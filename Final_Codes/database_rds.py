# -*- coding: utf-8 -*-
"""Database_rds.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1klwlbJ8vFXDfEx6Axv9a0-O4ESiaXMKl
"""

# Install Java, Spark, and Findspark
!apt-get install openjdk-8-jdk-headless -qq > /dev/null
!wget -q http://www-us.apache.org/dist/spark/spark-2.4.5/spark-2.4.5-bin-hadoop2.7.tgz
!tar xf spark-2.4.5-bin-hadoop2.7.tgz
!pip install -q findspark
import os
# Find the latest version of spark 2.0  from http://www-us.apache.org/dist/spark/ and enter as the spark version
# For example:
# spark_version = 'spark-2.4.6'
spark_version = 'spark-2.4.7'
os.environ['SPARK_VERSION']=spark_version
# Install Spark and Java
!apt-get update
!apt-get install openjdk-8-jdk-headless -qq > /dev/null
!wget -q http://www-us.apache.org/dist/spark/$SPARK_VERSION/$SPARK_VERSION-bin-hadoop2.7.tgz
!tar xf $SPARK_VERSION-bin-hadoop2.7.tgz
!pip install -q findspark
# Set Environment Variables
os.environ["JAVA_HOME"] = "/usr/lib/jvm/java-8-openjdk-amd64"
os.environ["SPARK_HOME"] = f"/content/{spark_version}-bin-hadoop2.7"

# Start a SparkSession
import findspark
findspark.init()

!wget https://jdbc.postgresql.org/download/postgresql-42.2.9.jar

from pyspark.sql import SparkSession
spark = SparkSession.builder.appName("CloudETL").config("spark.driver.extraClassPath","/content/postgresql-42.2.9.jar").getOrCreate()

# Read in data from S3 Buckets
from pyspark import SparkFiles
url="https://mahin.s3.amazonaws.com/Cross_Tb1.csv"
spark.sparkContext.addFile(url)
TB1 = spark.read.csv(SparkFiles.get("Cross_Tb1.csv"), sep=",", header=True, inferSchema=True)

# Show DataFrame
TB1.show()

url="https://mahin.s3.amazonaws.com/Cross_Tb2.csv"
spark.sparkContext.addFile(url)
TB2 = spark.read.csv(SparkFiles.get("Cross_Tb2.csv"), sep=",", header=True, inferSchema=True)

url="https://mahin.s3.amazonaws.com/Cross_Tb3.csv"
spark.sparkContext.addFile(url)
TB3 = spark.read.csv(SparkFiles.get("Cross_Tb3.csv"), sep=",", header=True, inferSchema=True)

url="https://mahin.s3.amazonaws.com/Cross_Tb4.csv"
spark.sparkContext.addFile(url)
TB4 = spark.read.csv(SparkFiles.get("Cross_Tb4.csv"), sep=",", header=True, inferSchema=True)

url="https://mahin.s3.amazonaws.com/Cross_Tb5.csv"
spark.sparkContext.addFile(url)
TB5 = spark.read.csv(SparkFiles.get("Cross_Tb5.csv"), sep=",", header=True, inferSchema=True)

url="https://mahin.s3.amazonaws.com/Cross_Tb7.csv"
spark.sparkContext.addFile(url)
TB7 = spark.read.csv(SparkFiles.get("Cross_Tb7.csv"), sep=",", header=True, inferSchema=True)

url="https://mahin.s3.amazonaws.com/Cross_Tb10.csv"
spark.sparkContext.addFile(url)
TB10 = spark.read.csv(SparkFiles.get("Cross_Tb10.csv"), sep=",", header=True, inferSchema=True)

url="https://mahin.s3.amazonaws.com/Cross_Tb11.csv"
spark.sparkContext.addFile(url)
TB11 = spark.read.csv(SparkFiles.get("Cross_Tb11.csv"), sep=",", header=True, inferSchema=True)

url="https://mahin.s3.amazonaws.com/Cross_Tb12.csv"
spark.sparkContext.addFile(url)
TB12 = spark.read.csv(SparkFiles.get("Cross_Tb12.csv"), sep=",", header=True, inferSchema=True)

url="https://mahin.s3.amazonaws.com/Cross_Tb13.csv"
spark.sparkContext.addFile(url)
TB13 = spark.read.csv(SparkFiles.get("Cross_Tb13.csv"), sep=",", header=True, inferSchema=True)

url="https://mahin.s3.amazonaws.com/Cross_Tb15.csv"
spark.sparkContext.addFile(url)
TB15 = spark.read.csv(SparkFiles.get("Cross_Tb15.csv"), sep=",", header=True, inferSchema=True)

url="https://mahin.s3.amazonaws.com/Cross_Tb17.csv"
spark.sparkContext.addFile(url)
TB17 = spark.read.csv(SparkFiles.get("Cross_Tb17.csv"), sep=",", header=True, inferSchema=True)

url="https://mahin.s3.amazonaws.com/Cross_Tb20.csv"
spark.sparkContext.addFile(url)
TB20 = spark.read.csv(SparkFiles.get("Cross_Tb20.csv"), sep=",", header=True, inferSchema=True)

url="https://mahin.s3.amazonaws.com/Cross_Tb21.csv"
spark.sparkContext.addFile(url)
TB21 = spark.read.csv(SparkFiles.get("Cross_Tb21.csv"), sep=",", header=True, inferSchema=True)

url="https://mahin.s3.amazonaws.com/Cross_Tb22.csv"
spark.sparkContext.addFile(url)
TB22 = spark.read.csv(SparkFiles.get("Cross_Tb22.csv"), sep=",", header=True, inferSchema=True)

url="https://mahin.s3.amazonaws.com/Cross_Tb23.csv"
spark.sparkContext.addFile(url)
TB23 = spark.read.csv(SparkFiles.get("Cross_Tb23.csv"), sep=",", header=True, inferSchema=True)

url="https://mahin.s3.amazonaws.com/Cross_Tb25.csv"
spark.sparkContext.addFile(url)
TB25 = spark.read.csv(SparkFiles.get("Cross_Tb25.csv"), sep=",", header=True, inferSchema=True)

url="https://mahin.s3.amazonaws.com/Cross_Tb27.csv"
spark.sparkContext.addFile(url)
TB27 = spark.read.csv(SparkFiles.get("Cross_Tb27.csv"), sep=",", header=True, inferSchema=True)

url="https://mahin.s3.amazonaws.com/Cross_Tb28.csv"
spark.sparkContext.addFile(url)
TB28 = spark.read.csv(SparkFiles.get("Cross_Tb28.csv"), sep=",", header=True, inferSchema=True)

url="https://mahin.s3.amazonaws.com/Cross_Tb29.csv"
spark.sparkContext.addFile(url)
TB29 = spark.read.csv(SparkFiles.get("Cross_Tb9.csv"), sep=",", header=True, inferSchema=True)

url="https://mahin.s3.amazonaws.com/Cross_Tb30.csv"
spark.sparkContext.addFile(url)
TB30 = spark.read.csv(SparkFiles.get("Cross_Tb30.csv"), sep=",", header=True, inferSchema=True)

url="https://mahin.s3.amazonaws.com/Cross_Tb31.csv"
spark.sparkContext.addFile(url)
TB31 = spark.read.csv(SparkFiles.get("Cross_Tb31.csv"), sep=",", header=True, inferSchema=True)

"""Postgres Setup"""

# Configure settings for RDS
mode = "append"
jdbc_url="jdbc:postgresql://database-1.c39jpegkslna.us-east-2.rds.amazonaws.com:5432/clinicaltrials_db"
config = {"user":"root", 
          "password": "12345678", 
          "driver":"org.postgresql.Driver"}

# Write DataFrame to tb1 table in RDS
TB1.write.jdbc(url=jdbc_url, table='tb1', mode=mode, properties=config)

TB2.write.jdbc(url=jdbc_url, table='tb2', mode=mode, properties=config)

TB3.write.jdbc(url=jdbc_url, table='tb3', mode=mode, properties=config)

TB4.write.jdbc(url=jdbc_url, table='tb4', mode=mode, properties=config)

TB5.write.jdbc(url=jdbc_url, table='tb5', mode=mode, properties=config)

TB7.write.jdbc(url=jdbc_url, table='tb7', mode=mode, properties=config)

TB10.write.jdbc(url=jdbc_url, table='tb10', mode=mode, properties=config)

TB11.write.jdbc(url=jdbc_url, table='tb11', mode=mode, properties=config)

TB12.write.jdbc(url=jdbc_url, table='tb12', mode=mode, properties=config)

TB13.write.jdbc(url=jdbc_url, table='tb13', mode=mode, properties=config)

TB15.write.jdbc(url=jdbc_url, table='tb15', mode=mode, properties=config)

TB17.write.jdbc(url=jdbc_url, table='tb17', mode=mode, properties=config)

TB20.write.jdbc(url=jdbc_url, table='tb20', mode=mode, properties=config)20

TB21.write.jdbc(url=jdbc_url, table='tb21', mode=mode, properties=config)

TB22.write.jdbc(url=jdbc_url, table='tb22', mode=mode, properties=config)

TB22.write.jdbc(url=jdbc_url, table='tb22', mode=mode, properties=config)

TB23.write.jdbc(url=jdbc_url, table='tb23', mode=mode, properties=config)

TB25.write.jdbc(url=jdbc_url, table='tb25, mode=mode, properties=config)

TB27.write.jdbc(url=jdbc_url, table='tb27', mode=mode, properties=config)

TB28.write.jdbc(url=jdbc_url, table='tb28', mode=mode, properties=config)

TB29.write.jdbc(url=jdbc_url, table='tb29', mode=mode, properties=config)

TB30.write.jdbc(url=jdbc_url, table='tb30', mode=mode, properties=config)

TB31.write.jdbc(url=jdbc_url, table='tb31', mode=mode, properties=config)

import psycopg2
from sqlalchemy import create_engine

conn = psycopg2.connect(host="database-1.c39jpegkslna.us-east-2.rds.amazonaws.com", port = 5432, database="clinicaltrials_db", user="root", password="12345678")

cur = conn.cursor()

cur.execute("""SELECT * FROM tb2 """)
query_results = cur.fetchall()
#print(query_results)
query_results[:20]

import pandas as pd

df2= pd.read_sql("""SELECT * FROM tb2 """, conn)
df2.head()